{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste BB\n",
    "### Dataset Census.csv\n",
    "1. Coluna age: idade da pessoa que participou da avaliação de quanto ganha anualmente;\n",
    "\n",
    "2. Coluna worclass: é qual o setor que a pessoa trabalha;\n",
    "\n",
    "3. Coluna education_level: qual é o nível de instrução do usuário, segue abaixo os tipos que serão encontrados e válidos:\n",
    "\n",
    "a. Bachelors;\n",
    "\n",
    "b. Masters;\n",
    "\n",
    "c. HS-grad;\n",
    "\n",
    "d. Some-college;\n",
    "\n",
    "e. Doctorate.\n",
    "\n",
    "4. Coluna education_num: número da educação, é o processo de quantificar o nível educacional e dar um peso para seu identificador;\n",
    "\n",
    "5. Coluna marital-status: o status de matrimônio do usuário que foi entrevistado;\n",
    "\n",
    "6. Coluna occupation: ocupação do usuário que foi entrevistado, vale lembrar que essa variável pertence ao estado irregular;\n",
    "\n",
    "7. Coluna relationship: é a coluna de relacionamento, onde detalha seu grau de relacionamento atual, segue as variáveis válidas:\n",
    "\n",
    "a. Not-in-family;\n",
    "\n",
    "b. Husband;\n",
    "\n",
    "c. Wife;\n",
    "\n",
    "d. Own-child;\n",
    "\n",
    "e. Unmarried.\n",
    "\n",
    "8. Coluna race: coluna que demonstra a tonalidade da pele do usuário, as variáveis válidas são:\n",
    "\n",
    "a. White;\n",
    "\n",
    "b. Black;\n",
    "\n",
    "c. Asian-Pac-Islander;\n",
    "\n",
    "d. Amer-Indian-Eskimo;\n",
    "\n",
    "e. Other.\n",
    "\n",
    "9. Coluna sex: o sexo da pessoa (Masculino ou Feminino);\n",
    "\n",
    "10. Coluna capital-gain: capital ganho ou adquirido;\n",
    "\n",
    "11. Coluna capital-loss: capital perdido ou não conquistado;\n",
    "\n",
    "12. Coluna hours-per-week: horas por semana trabalhadas;\n",
    "\n",
    "13. Coluna native-country: País nativo do usuário que foi entrevistado;\n",
    "\n",
    "14. Coluna income: a renda anual do usuário que foi entrevistado, saídas:\n",
    "a. Menos que 50k;\n",
    "b. Menor e igual a 50k;\n",
    "c. Maior que 50k.\n",
    "\n",
    "\n",
    "## Saídas esperadas\n",
    "\n",
    "Para este desafio esperamos algumas saídas necessárias, conforme abaixo (via TXT ou Markdown):\n",
    "\n",
    "● Explicar as variáveis que foram exploradas e os motivos;\n",
    "\n",
    "● Caso tenha sido necessário novas variáveis, explicar o motivo da criação;\n",
    "\n",
    "● Explicar o processo de preparação;\n",
    "\n",
    "● Detalhar se houve algum processo que deve ser considerado transformação de recursos contínuos enviesados e o porquê;\n",
    "\n",
    "● Explicar qual foi o processo utilizado para normalização dos dados;\n",
    "\n",
    "● Detalhar qual foi o modelo utilizado para desempenho e predição dos dados;\n",
    "\n",
    "● Se chegou a utilizar Modelos de Aprendizagem Supervisionados, explique quais;\n",
    "\n",
    "● Detalhar o que foi utilizado no treino e de evolução;\n",
    "\n",
    "● Apresentar a acuracidade do modelo e a revalidação do modelo com Log Loss.\n",
    "\n",
    "O arquivo gerado com as saídas solicitadas acima deve ser armazenados em seu repositório pessoal doGitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot  as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros de visualização de dados\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "rc={'savefig.dpi': 75, 'figure.autolayout': False, 'figure.figsize': [12, 8], 'axes.labelsize': 18,\\\n",
    "   'axes.titlesize': 18, 'font.size': 18, 'lines.linewidth': 2.0, 'lines.markersize': 8, 'legend.fontsize': 16,\\\n",
    "   'xtick.labelsize': 16, 'ytick.labelsize': 16}\n",
    "\n",
    "sns.set(style='dark',rc=rc)\n",
    "default_color = '#56B4E9'\n",
    "colormap = plt.cm.cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('census.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze outliers\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze columns types and NAN values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm NAN\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#details NAN\n",
    "df[df['capital-gain'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.countplot('income',data=df,color=default_color)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(df['income'])), (p.get_x()+ 0.3, p.get_height()+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze frequency values per columns\n",
    "for col in df.columns:\n",
    "    print('columns:{}\\ntype:{}\\nqtd_unique: {}\\nvalue_unique: {}\\n'.format(col, df[col].dtype, df[col].nunique(),df[col].unique()  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesta analise verificou-se que existem na coluna 'education_level', 16 frequência de valores, onde no enunciado do teste permite somente cinco variavéis váliddas – a. Bachelors; b. Masters; c. HS-grad; d. Some-college; e. Doctorate –, bem como, ocorre com a coluna relationship tendo uma variavel ('Other-relative'). \n",
    "#### Já a coluna 'sex' possui um possivel erro de digitação na variável('mal'), ocorre que este erro esta na mesma linha que contém 5 NAN,sendo descartado esta linha. Lamento que estas ocorrências tenham um custo alto para o Dataset, retirando 23,98% do total de dados.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask of valid variables and elimination of invalid rows\n",
    "mask1 = df['education_level'] == ' Bachelors'\n",
    "mask2 = df['education_level'] == ' HS-grad'\n",
    "mask3 = df['education_level'] == ' Masters'\n",
    "mask4 = df['education_level'] == ' Some-college'\n",
    "mask5 = df['education_level'] == ' Doctorate'\n",
    "df = df[mask1 | mask2 | mask3 | mask4 | mask5]\n",
    "df = df[df.relationship != ' Other-relative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A coluna education_level poderá ser excluida porque possui a coluna education-num que possui os mesmo dados em formato ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column education_level\n",
    "df.drop('education_level', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the column types according to the variables of the values\n",
    "\n",
    "df['workclass'] = df['workclass'].astype('category')\n",
    "df['marital-status'] = df['marital-status'].astype('category')\n",
    "df['occupation'] = df['occupation'].astype('category')\n",
    "df['relationship'] = df['relationship'].astype('category')\n",
    "df['race'] = df['race'].astype('category')\n",
    "df['sex'] = df['sex'].astype('category')\n",
    "df['native-country'] = df['native-country'].astype('category')\n",
    "df['education-num'] = df['education-num'].astype('int64')\n",
    "\n",
    "#Encode the 'income' data to numerical values\n",
    "df['income'] = df['income'].apply(lambda x: 1 if x == \">50K\" else 0)\n",
    "\n",
    "# Drop NAN lines\n",
    "df.drop(5532, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(train):\n",
    "    data = []\n",
    "    for col in train.columns:\n",
    "        # Defining the role\n",
    "        if col == 'income':\n",
    "            role = 'target'\n",
    "        else:\n",
    "            role = 'input'\n",
    "\n",
    "        # Defining the level\n",
    "        if col == 'income':\n",
    "            level = 'binary'\n",
    "        elif train[col].dtype == np.float64:\n",
    "            level = 'interval'\n",
    "        elif train[col].dtype == np.int64:\n",
    "            level = 'ordinal'\n",
    "        elif train[col].dtype == 'category':\n",
    "            level = 'category'\n",
    "\n",
    "        # Initialize keep to True for all variables except for id\n",
    "        keep = True\n",
    "        if col == 'id':\n",
    "            keep = False\n",
    "\n",
    "        # Defining the data type \n",
    "        dtype = train[col].dtype\n",
    "\n",
    "        # Creating a Dict that contains all the metadata for the variable\n",
    "        col_dict = {\n",
    "            'varname': col,\n",
    "            'role'   : role,\n",
    "            'level'  : level,\n",
    "            'keep'   : keep,\n",
    "            'dtype'  : dtype\n",
    "        }\n",
    "        data.append(col_dict)\n",
    "    meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n",
    "    meta.set_index('varname', inplace=True)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = get_meta(df)\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By DataType\n",
    "meta_counts = meta_data.groupby(['role', 'level']).agg({'dtype': lambda x: x.count()}).reset_index()\n",
    "meta_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Variables Count Across Datatype\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(20,5)\n",
    "sns.barplot(data=meta_counts[(meta_counts.role != 'target')],x=\"level\",y=\"dtype\",ax=ax,color=default_color)\n",
    "ax.set(xlabel='Variable Type', ylabel='Count',title=\"Variables Count Across Datatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ordinal   = meta_data[(meta_data.level == 'ordinal') & (meta_data.keep)].index\n",
    "col_category   = meta_data[(meta_data.level == 'category') & (meta_data.keep)].index\n",
    "col_interval = meta_data[(meta_data.level == 'interval') & (meta_data.keep)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there is continuous data that has a strong correlation with Person correlation\n",
    "plt.figure(figsize=(18,16))\n",
    "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n",
    "sns.heatmap(df[col_interval].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['income']\n",
    "previsores = df.drop('income', axis = 1)\n",
    "\n",
    "\n",
    "#I created the variable features to test the importance of features with RandonForest\n",
    "features = df.drop('income', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#previsores[col_ordinal] = scaler.fit_transform(df[col_ordinal])\n",
    "previsores[col_interval] = scaler.fit_transform(df[col_interval])\n",
    "features[col_interval] = scaler.fit_transform(df[col_interval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncoder to prepare the cathegorical data in formats accepted by the algorithms\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_previsores = LabelEncoder()\n",
    "for col in col_category:\n",
    "    features[col] = labelencoder_previsores.fit_transform(features[col])\n",
    "    previsores[col] = labelencoder_previsores.fit_transform(previsores[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing feature importantes with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n",
    "rf.fit(features, target)\n",
    "features_importantes = features.columns.values\n",
    "print(\"----- Training Done -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_df(feature_importances, \n",
    "                              column_names, \n",
    "                              top_n=25):\n",
    "    \"\"\"Get feature importance data frame.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_importances : numpy ndarray\n",
    "        Feature importances computed by an ensemble \n",
    "            model like random forest or boosting\n",
    "    column_names : array-like\n",
    "        Names of the columns in the same order as feature \n",
    "            importances\n",
    "    top_n : integer\n",
    "        Number of top features\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    df : a Pandas data frame\n",
    " \n",
    "    \"\"\"\n",
    "     \n",
    "    imp_dict = dict(zip(column_names, \n",
    "                        feature_importances))\n",
    "    top_features = sorted(imp_dict, \n",
    "                          key=imp_dict.get, \n",
    "                          reverse=True)[0:top_n]\n",
    "    top_importances = [imp_dict[feature] for feature \n",
    "                          in top_features]\n",
    "    df = pd.DataFrame(data={'feature': top_features, \n",
    "                            'importance': top_importances})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = get_feature_importance_df(rf.feature_importances_, features_importantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "sns.barplot(data=feature_importance[:10],x=\"feature\",y=\"importance\",ax=ax,color=default_color,)\n",
    "ax.set(xlabel='Variable name', ylabel='Importance',title=\"Variable importances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(previsores, target, test_size = 0.2, random_state = 0)\n",
    "X_train.shape\n",
    "X_test.shape\n",
    "print('Train: {}\\nTest:{}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  accuracy_score, log_loss\n",
    "\n",
    "def avaliacao(y_test, previ , pred_proba):\n",
    "    \n",
    "    precisao = accuracy_score(y_test, previ)\n",
    "    print('accuracy: {}\\n'.format(precisao))\n",
    "    print('Log_loss: {}\\n\\n'.format(log_loss(y_test,pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def treinamento(X_train, X_test, y_train, y_test, alg):\n",
    "    \n",
    "    \n",
    "    def a_naivebayes(X_train, X_test, y_train, y_test):\n",
    "        classificador = GaussianNB()\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('Naive Bayes')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "    def a_knn(X_train, X_test, y_train, y_test):\n",
    "        classificador = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('KNN')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "    def a_rf(X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        rf_params = {}\n",
    "        rf_params['n_estimators'] = 200\n",
    "        rf_params['random_state'] = 0\n",
    "        rf_params['criterion'] = 'entropy'\n",
    "        rf_params['max_depth'] = 6\n",
    "        rf_params['min_samples_split'] = 70\n",
    "        rf_params['min_samples_leaf'] = 30\n",
    "        classificador = RandomForestClassifier(**rf_params)\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('RandomForest')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "    def a_lr(X_train, X_test, y_train, y_test):\n",
    "        classificador = LogisticRegression()\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('LogisticRegression')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "    \n",
    "    def a_svm(X_train, X_test, y_train, y_test):\n",
    "        classificador = SVC(kernel = 'linear', random_state = 1, probability= True)\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('SVM')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "    def a_keras(X_train, X_test, y_train, y_test):\n",
    "        classificador = Sequential()\n",
    "        classificador.add(Dense(units = round((X_train.shape[1]+1)/2), activation = 'relu', input_dim = X_train.shape[1]))\n",
    "        classificador.add(Dense(units = round((X_train.shape[1]+1)/2), activation = 'relu'))\n",
    "        classificador.add(Dense(units = 21, activation = 'relu'))\n",
    "        classificador.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        classificador.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "        #print(classificador.summary())\n",
    "\n",
    "\n",
    "        classificador.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        previsoes = (previsoes > 0.5)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        pred_proba = pred_proba.astype(np.float64)\n",
    "\n",
    "\n",
    "        print('NeuralNetwork Keras')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "    def a_xgb(X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        # XGBoost params# XGBoos \n",
    "        xgb_params = {}\n",
    "        xgb_params['learning_rate'] = 0.02\n",
    "        xgb_params['n_estimators'] = 1000\n",
    "        xgb_params['max_depth'] = 4\n",
    "        xgb_params['subsample'] = 0.9\n",
    "        xgb_params['colsample_bytree'] = 0.9\n",
    "        classificador = XGBClassifier(**xgb_params)\n",
    "        classificador.fit(X_train, y_train)\n",
    "        previsoes = classificador.predict(X_test)\n",
    "        pred_proba = classificador.predict_proba(X_test)\n",
    "        print('XGBoost')\n",
    "        avaliacao(y_test, previsoes, pred_proba)\n",
    "        \n",
    "\n",
    "    if alg == 'naive_bayes':\n",
    "        a_naivebayes(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'knn':\n",
    "        a_knn(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'rf':\n",
    "        a_rf(X_train, X_test, y_train, y_test)        \n",
    "    elif alg == 'lr':\n",
    "        a_lr(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'svm':\n",
    "        a_svm(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'keras':\n",
    "        a_keras(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'xgb':\n",
    "        a_xgb(X_train, X_test, y_train, y_test)\n",
    "    elif alg == 'all':\n",
    "        a_naivebayes(X_train, X_test, y_train, y_test)\n",
    "        a_knn(X_train, X_test, y_train, y_test)\n",
    "        a_rf(X_train, X_test, y_train, y_test)\n",
    "        a_lr(X_train, X_test, y_train, y_test)\n",
    "        a_svm(X_train, X_test, y_train, y_test)\n",
    "        a_keras(X_train, X_test, y_train, y_test)\n",
    "        a_xgb(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados realizando a limpeza de dados, escalonando as variaveis no dataset, com excessão das target e das colunas com types ordinais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento(X_train, X_test, y_train, y_test, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "#### One-hot Encoding effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "target = df['income']\n",
    "previsores = df.drop('income', axis = 1)\n",
    "previsores[col_interval] = scaler.fit_transform(df[col_interval])\n",
    "previsores = pd.get_dummies(previsores)\n",
    "print(\"total features after one-hot: {}\".format(len(list(previsores.columns))))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(previsores, target, test_size = 0.2, random_state = 0)\n",
    "X_train.shape\n",
    "X_test.shape\n",
    "print('Train: {}\\nTest:{}'.format(len(X_train), len(X_test)))\n",
    "previsores.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento(X_train, X_test, y_train, y_test, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features effect\n",
    "\n",
    "\n",
    "### Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "target = df['income']\n",
    "previsores = df.drop('income', axis = 1)\n",
    "#previsores['bin_sum']  = previsores[col_category].sum(axis=1)\n",
    "previsores['ord_sum']  = previsores[col_ordinal].sum(axis=1)\n",
    "previsores['intenval_median']  = previsores[col_interval].sum(axis=1)\n",
    "\n",
    "previsores[col_interval] = scaler.fit_transform(df[col_interval])\n",
    "previsores = pd.get_dummies(previsores)\n",
    "print(\"total features after one-hot: {}\".format(len(list(previsores.columns))))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(previsores, target, test_size = 0.2, random_state = 0)\n",
    "X_train.shape\n",
    "X_test.shape\n",
    "print('Train: {}\\nTest:{}'.format(len(X_train), len(X_test)))\n",
    "previsores.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento(X_train, X_test, y_train, y_test, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
